{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB User Reviews Review Scraper\n",
    "\n",
    "This code is written by taking the help of <br>\n",
    "https://hungpham89.medium.com/a-beginner-guide-for-scraping-data-from-imdb-for-user-reviews-using-selenium-and-beautifulsoup-c60e89a4ad1a\n",
    "<br>\n",
    "IMDB's website structure has changed a bit since Hung Pham wrote this code. So I had to do some modifications to the code. Now it is running fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step, we will need to import all the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #Using panda to create our dataframe\n",
    "# Import Selenium and its sub libraries\n",
    "import selenium \n",
    "from selenium import webdriver\n",
    "# Import BS4\n",
    "import requests #needed to load the page for BS4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from IPython.display import clear_output\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we chose Chrome as our main web browser, we will need to download Chrome driver and tell Selenium where to find it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r\"D:\\temp\\chromedriver.exe\" #path to the webdriver file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the below dataset variable will contain all review information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe for storing reviews\n",
    "dataset = pd.DataFrame(columns = ['User_name', \n",
    "        'Review title', \n",
    "        'Review Rating',\n",
    "        'Review date',\n",
    "        'Review_body',\n",
    "        'Movie_name',\n",
    "        'Movie_ID',\n",
    "         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will fetch reviews from a link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review(url):\n",
    "\n",
    "    '''\n",
    "    Get the review from input as url for IMDB movies list.\n",
    "    The function takes 2 input the url of the movies and the name of the folder to store the data\n",
    "    For each folder, the function will grab the review for each movies and store into respective file.\n",
    "    '''\n",
    "    global dataset\n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--dns-prefetch-disable\") # for avoiding timeout error\n",
    "    driver = webdriver.Chrome(PATH,chrome_options=chrome_options) #tell selenium to use Chrome and find the webdriver file in this location\n",
    "    driver.get(url) #tell Selenium to open the webpage\n",
    "    driver.implicitly_wait(1) # tell the webdriver to wait for 1 seconds for the page to load\n",
    "\n",
    "    #After the webpage opened, we can extract the title, hyperlink, year of each movies\n",
    "    #Set initial empty list for each element:\n",
    "    title = []\n",
    "    link = []\n",
    "    year = []\n",
    "\n",
    "    #Grab the block of each individual movie\n",
    "    block = driver.find_elements_by_class_name('lister-item')\n",
    "    #Set up for loop to run through all 250 movies in the first page\n",
    "    for i in range(0,250):\n",
    "        try:\n",
    "            #Extracting title\n",
    "            ftitle = block[i].find_element_by_class_name('lister-item-header').text\n",
    "\n",
    "            #The extracted title has extra elements, so we will have to do some cleaning\n",
    "            #Remove the order in front of the title\n",
    "            forder = block[i].find_element_by_class_name('lister-item-index').text\n",
    "            #Extract the year last 6 letter of the title\n",
    "            fyear = ftitle[-6:]\n",
    "            #Drop the order, year and only keep the movie's name\n",
    "            ftitle = ftitle.replace(forder+' ', '')[:-7 ]\n",
    "            #Then extract the link with cleaned title\n",
    "            flink = block[i].find_element_by_link_text(ftitle).get_attribute('href')\n",
    "\n",
    "            #Add item to the respective lists\n",
    "            title.append(ftitle)\n",
    "            year.append(fyear)\n",
    "            link.append(flink)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # After that, we can use BeautifulSoup to extract the user reviews link \n",
    "    #Set an empty list to store user review link\n",
    "    user_review_links = []\n",
    "    IMDB_IDs = []\n",
    "    for url in link:\n",
    "        url = url\n",
    "        #setup user agent for BS4, except some rare case, it would be the same for most browser \n",
    "        user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "        #Use request.get to load the whole page\n",
    "        response = requests.get(url, headers = user_agent)\n",
    "        #Parse the request object to BS4 to transform it into html structure\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        #Create a link like https://www.imdb.com/title/tt5354160/reviews/?ref_=tt_ql_urv\n",
    "        review_link = 'https://www.imdb.com/title/'+url[27:url.find('?')-1]+'/reviews/'\n",
    "        #Append the newly grabed link into its list\n",
    "        user_review_links.append(review_link)\n",
    "        # fetch imdb id of movie from URL\n",
    "        id = url[27:url.find('?')-1]\n",
    "        # append IMDB ID to list\n",
    "        IMDB_IDs.append(id)\n",
    "\n",
    "    #Then create the first data frame to summarize our data at this point:\n",
    "    #Create dictionary for data and columns' name\n",
    "    movie_data = {'Movie_name': title, \n",
    "            'Year': year, \n",
    "            'link': link,\n",
    "            'user_review' : user_review_links,\n",
    "            'Movie_ID' : IMDB_IDs,\n",
    "            }\n",
    "    movies = pd.DataFrame(data = movie_data) #create dataframe\n",
    "    driver.quit() #tell Selenium to close the webpage\n",
    "\n",
    "    # Step 2, we will grab the data from each user review page\n",
    "    # Use Selenium to go to each user review page\n",
    "    for i in range(len(movies['user_review'])): \n",
    "        print(\"collecting data of movie \",i)\n",
    "        driver = webdriver.Chrome(PATH)\n",
    "        driver.get(movies['user_review'][i])\n",
    "        driver.implicitly_wait(1) # tell the webdriver to wait for 1 seconds for the page to load to prevent blocked by anti spam software\n",
    "\n",
    "\n",
    "        # Set up action to click on 'load more' button\n",
    "        # note that each page on imdb has 25 reviews\n",
    "        page = 1 #Set initial variable for while loop\n",
    "        #We want at least 1000 review, so get 50 at a safe number\n",
    "        while page<50:  \n",
    "            try:\n",
    "                #find the load more button on the webpage\n",
    "                load_more = driver.find_element_by_id('load-more-trigger')\n",
    "                #click on that button\n",
    "                load_more.click()\n",
    "                page+=1 #move on to next loadmore button\n",
    "            except:\n",
    "                #If couldnt find any button to click, stop\n",
    "                break\n",
    "        # After fully expand the page, we will grab data from whole website\n",
    "        review = driver.find_elements_by_class_name('review-container')\n",
    "        #Set list for each element:\n",
    "        title = []\n",
    "        content = []\n",
    "        rating = []\n",
    "        date = []\n",
    "        user_name = []\n",
    "        #run for loop to get reviews\n",
    "        for n in range(0,70): # collecting at most 70 reviews for each movie\n",
    "            try:\n",
    "                #Some reviewers only give review text or rating without the other, \n",
    "                #so we use try/except here to make sure each block of content must has all the element before append them to the list\n",
    "\n",
    "                #Check if each review has all the elements\n",
    "                ftitle = review[n].find_element_by_class_name('title').text\n",
    "                #For the review content, some of them are hidden as spoiler, \n",
    "                #so we use the attribute 'textContent' here after extracting the 'content' tag\n",
    "                fcontent = review[n].find_element_by_class_name('content').get_attribute(\"textContent\").strip()\n",
    "                frating = review[n].find_element_by_class_name('rating-other-user-rating').text[0]\n",
    "                fdate = review[n].find_element_by_class_name('review-date').text\n",
    "                fname = review[n].find_element_by_class_name('display-name-link').text\n",
    "\n",
    "\n",
    "                #Then add them to the respective list\n",
    "                title.append(ftitle)\n",
    "                content.append(fcontent)\n",
    "                rating.append(frating)\n",
    "                date.append(fdate)\n",
    "                user_name.append(fname)\n",
    "            except:\n",
    "                continue\n",
    "        #Build data dictionary for dataframe\n",
    "        data = {'User_name': user_name, \n",
    "            'Review title': title, \n",
    "            'Review Rating': rating,\n",
    "            'Review date' : date,\n",
    "            'Review_body' : content,\n",
    "            'Movie_name' : [movies['Movie_name'][i] for p in range(len(title))], #grab the movie name from the movies list \n",
    "            'Movie_ID' : [movies['Movie_ID'][i] for p in range(len(title))], # grab imdb id from movies list\n",
    "           }\n",
    "        #Build dataframe for each movie to export\n",
    "        review = pd.DataFrame(data = data)\n",
    "        dataset = pd.concat([dataset,review],ignore_index=True) #append review to dataset\n",
    "        driver.quit()\n",
    "        clear_output(wait=True) # for clearing console\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can save the newly created raw dataframe into a csv file for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the function call below will start the scraping process. it takes about 15 seconds to scrape data for each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting data of movie  59\n"
     ]
    }
   ],
   "source": [
    "# An array to store all the URL that are being queried\n",
    "imageArr = []\n",
    "\n",
    "# Maximum number of pages one wants to iterate over\n",
    "MAX_PAGE =10\n",
    "\n",
    "# Loop to generate all the URLS.\n",
    "for i in range(0,MAX_PAGE):\n",
    "    totalRecords = 0 if i==0 else (250*i)+1\n",
    "    imdb_url = f'https://www.imdb.com/search/title/?languages=bn&sort=year,desc&user_rating=1.0,10.0&title_type=feature&count=250&start={totalRecords}&ref_=adv_nxt'\n",
    "    imageArr.append(imdb_url)\n",
    "\n",
    "# now we call all the URLs to collect data\n",
    "for bangla_movies_link in imageArr:\n",
    "    get_review(bangla_movies_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('bangla movie user rating dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
