{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB User Reviews Review Scraper\n",
    "\n",
    "This code is written by taking the help of <br>\n",
    "https://hungpham89.medium.com/a-beginner-guide-for-scraping-data-from-imdb-for-user-reviews-using-selenium-and-beautifulsoup-c60e89a4ad1a\n",
    "<br>\n",
    "IMDB's website structure has changed a bit since Hung Pham wrote this code. So I had to do some modifications to the code. Now it is running fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step, we will need to import all the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #Using panda to create our dataframe\n",
    "# Import Selenium and its sub libraries\n",
    "import selenium \n",
    "from selenium import webdriver\n",
    "# Import BS4\n",
    "import requests #needed to load the page for BS4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we chose Chrome as our main web browser, we will need to download Chrome driver and tell Selenium where to find it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r\"D:\\temp\\chromedriver.exe\" #path to the webdriver file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the below dataset variable will contain all review information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe for storing reviews\n",
    "dataset = pd.DataFrame(columns = ['User_name', \n",
    "        'Review title', \n",
    "        'Review Rating',\n",
    "        'Review date',\n",
    "        'Review_body',\n",
    "        'Movie_name',\n",
    "        'Movie_ID',\n",
    "         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will fetch reviews from a link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review(url):\n",
    "\n",
    "    '''\n",
    "    Get the review from input as url for IMDB movies list.\n",
    "    The function takes 2 input the url of the movies and the name of the folder to store the data\n",
    "    For each folder, the function will grab the review for each movies and store into respective file.\n",
    "    '''\n",
    "    global dataset\n",
    "\n",
    "    driver = webdriver.Chrome(PATH) #tell selenium to use Chrome and find the webdriver file in this location\n",
    "    driver.get(url) #tell Selenium to open the webpage\n",
    "    driver.implicitly_wait(1) # tell the webdriver to wait for 1 seconds for the page to load\n",
    "\n",
    "    #After the webpage opened, we can extract the title, hyperlink, year of each movies\n",
    "    #Set initial empty list for each element:\n",
    "    title = []\n",
    "    link = []\n",
    "    year = []\n",
    "\n",
    "    #Grab the block of each individual movie\n",
    "    block = driver.find_elements_by_class_name('lister-item')\n",
    "    #Set up for loop to run through all 250 movies in the first page\n",
    "    for i in range(0,250):\n",
    "        try:\n",
    "            #Extracting title\n",
    "            ftitle = block[i].find_element_by_class_name('lister-item-header').text\n",
    "\n",
    "            #The extracted title has extra elements, so we will have to do some cleaning\n",
    "            #Remove the order in front of the title\n",
    "            forder = block[i].find_element_by_class_name('lister-item-index').text\n",
    "            #Extract the year last 6 letter of the title\n",
    "            fyear = ftitle[-6:]\n",
    "            #Drop the order, year and only keep the movie's name\n",
    "            ftitle = ftitle.replace(forder+' ', '')[:-7 ]\n",
    "            #Then extract the link with cleaned title\n",
    "            flink = block[i].find_element_by_link_text(ftitle).get_attribute('href')\n",
    "\n",
    "            #Add item to the respective lists\n",
    "            title.append(ftitle)\n",
    "            year.append(fyear)\n",
    "            link.append(flink)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # After that, we can use BeautifulSoup to extract the user reviews link \n",
    "    #Set an empty list to store user review link\n",
    "    user_review_links = []\n",
    "    IMDB_IDs = []\n",
    "    for url in link:\n",
    "        url = url\n",
    "        #setup user agent for BS4, except some rare case, it would be the same for most browser \n",
    "        user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "        #Use request.get to load the whole page\n",
    "        response = requests.get(url, headers = user_agent)\n",
    "        #Parse the request object to BS4 to transform it into html structure\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        #Create a link like https://www.imdb.com/title/tt5354160/reviews/?ref_=tt_ql_urv\n",
    "        review_link = 'https://www.imdb.com/title/'+url[27:url.find('?')-1]+'/reviews/'\n",
    "        #Append the newly grabed link into its list\n",
    "        user_review_links.append(review_link)\n",
    "        # fetch imdb id of movie from URL\n",
    "        id = url[27:url.find('?')-1]\n",
    "        # append IMDB ID to list\n",
    "        IMDB_IDs.append(id)\n",
    "\n",
    "    #Then create the first data frame to summarize our data at this point:\n",
    "    #Create dictionary for data and columns' name\n",
    "    movie_data = {'Movie_name': title, \n",
    "            'Year': year, \n",
    "            'link': link,\n",
    "            'user_review' : user_review_links,\n",
    "            'Movie_ID' : IMDB_IDs,\n",
    "            }\n",
    "    movies = pd.DataFrame(data = movie_data) #create dataframe\n",
    "    driver.quit() #tell Selenium to close the webpage\n",
    "\n",
    "    # Step 2, we will grab the data from each user review page\n",
    "    # Use Selenium to go to each user review page\n",
    "    for i in range(len(movies['user_review'])): \n",
    "        print(\"collecting data of movie \",i)\n",
    "        driver = webdriver.Chrome(PATH)\n",
    "        driver.get(movies['user_review'][i])\n",
    "        driver.implicitly_wait(1) # tell the webdriver to wait for 1 seconds for the page to load to prevent blocked by anti spam software\n",
    "\n",
    "\n",
    "        # Set up action to click on 'load more' button\n",
    "        # note that each page on imdb has 25 reviews\n",
    "        page = 1 #Set initial variable for while loop\n",
    "        #We want at least 1000 review, so get 50 at a safe number\n",
    "        while page<50:  \n",
    "            try:\n",
    "                #find the load more button on the webpage\n",
    "                load_more = driver.find_element_by_id('load-more-trigger')\n",
    "                #click on that button\n",
    "                load_more.click()\n",
    "                page+=1 #move on to next loadmore button\n",
    "            except:\n",
    "                #If couldnt find any button to click, stop\n",
    "                break\n",
    "        # After fully expand the page, we will grab data from whole website\n",
    "        review = driver.find_elements_by_class_name('review-container')\n",
    "        #Set list for each element:\n",
    "        title = []\n",
    "        content = []\n",
    "        rating = []\n",
    "        date = []\n",
    "        user_name = []\n",
    "        #run for loop to get reviews\n",
    "        for n in range(0,70):\n",
    "            try:\n",
    "                #Some reviewers only give review text or rating without the other, \n",
    "                #so we use try/except here to make sure each block of content must has all the element before append them to the list\n",
    "\n",
    "                #Check if each review has all the elements\n",
    "                ftitle = review[n].find_element_by_class_name('title').text\n",
    "                #For the review content, some of them are hidden as spoiler, \n",
    "                #so we use the attribute 'textContent' here after extracting the 'content' tag\n",
    "                fcontent = review[n].find_element_by_class_name('content').get_attribute(\"textContent\").strip()\n",
    "                frating = review[n].find_element_by_class_name('rating-other-user-rating').text[0]\n",
    "                fdate = review[n].find_element_by_class_name('review-date').text\n",
    "                fname = review[n].find_element_by_class_name('display-name-link').text\n",
    "\n",
    "\n",
    "                #Then add them to the respective list\n",
    "                title.append(ftitle)\n",
    "                content.append(fcontent)\n",
    "                rating.append(frating)\n",
    "                date.append(fdate)\n",
    "                user_name.append(fname)\n",
    "            except:\n",
    "                continue\n",
    "        #Build data dictionary for dataframe\n",
    "        data = {'User_name': user_name, \n",
    "            'Review title': title, \n",
    "            'Review Rating': rating,\n",
    "            'Review date' : date,\n",
    "            'Review_body' : content,\n",
    "            'Movie_name' : [movies['Movie_name'][i] for p in range(len(title))], #grab the movie name from the movies list \n",
    "            'Movie_ID' : [movies['Movie_ID'][i] for p in range(len(title))], # grab imdb id from movies list\n",
    "           }\n",
    "        #Build dataframe for each movie to export\n",
    "        review = pd.DataFrame(data = data)\n",
    "        dataset = pd.concat([dataset,review],ignore_index=True) #append review to dataset\n",
    "        driver.quit()\n",
    "        clear_output(wait=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can save the newly created raw dataframe into a csv file for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the function call below will start the scraping process. it takes about 15 seconds to scrape data for each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting data of movie  174\n"
     ]
    },
    {
     "ename": "TimeoutException",
     "evalue": "Message: timeout: Timed out receiving message from renderer: 299.260\n  (Session info: chrome=96.0.4664.45)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x00CA6903+2517251]\n\tOrdinal0 [0x00C3F8E1+2095329]\n\tOrdinal0 [0x00B42848+1058888]\n\tOrdinal0 [0x00B34F05+1003269]\n\tOrdinal0 [0x00B33F8D+999309]\n\tOrdinal0 [0x00B34357+1000279]\n\tOrdinal0 [0x00B3DF7F+1040255]\n\tOrdinal0 [0x00B48249+1081929]\n\tOrdinal0 [0x00B4A90B+1091851]\n\tOrdinal0 [0x00B34635+1001013]\n\tOrdinal0 [0x00B48101+1081601]\n\tOrdinal0 [0x00B95E8E+1400462]\n\tOrdinal0 [0x00B8639B+1336219]\n\tOrdinal0 [0x00B627A7+1189799]\n\tOrdinal0 [0x00B63609+1193481]\n\tGetHandleVerifier [0x00E35904+1577972]\n\tGetHandleVerifier [0x00EE0B97+2279047]\n\tGetHandleVerifier [0x00D36D09+534521]\n\tGetHandleVerifier [0x00D35DB9+530601]\n\tOrdinal0 [0x00C44FF9+2117625]\n\tOrdinal0 [0x00C498A8+2136232]\n\tOrdinal0 [0x00C499E2+2136546]\n\tOrdinal0 [0x00C53541+2176321]\n\tBaseThreadInitThunk [0x766CFA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x774B7A9E+286]\n\tRtlGetAppContainerNamedObjectPath [0x774B7A6E+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-9275cc213ab7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# now we call all the URLs to collect data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbangla_movies_link\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimageArr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mget_review\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbangla_movies_link\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-2534810a9362>\u001b[0m in \u001b[0;36mget_review\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"collecting data of movie \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmovies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user_review'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimplicitly_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# tell the webdriver to wait for 1 seconds for the page to load to prevent blocked by anti spam software\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    434\u001b[0m         \u001b[0mLoads\u001b[0m \u001b[0ma\u001b[0m \u001b[0mweb\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m         \"\"\"\n\u001b[1;32m--> 436\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    426\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    245\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_KT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_VT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_KT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_VT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0m_VT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTimeoutException\u001b[0m: Message: timeout: Timed out receiving message from renderer: 299.260\n  (Session info: chrome=96.0.4664.45)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x00CA6903+2517251]\n\tOrdinal0 [0x00C3F8E1+2095329]\n\tOrdinal0 [0x00B42848+1058888]\n\tOrdinal0 [0x00B34F05+1003269]\n\tOrdinal0 [0x00B33F8D+999309]\n\tOrdinal0 [0x00B34357+1000279]\n\tOrdinal0 [0x00B3DF7F+1040255]\n\tOrdinal0 [0x00B48249+1081929]\n\tOrdinal0 [0x00B4A90B+1091851]\n\tOrdinal0 [0x00B34635+1001013]\n\tOrdinal0 [0x00B48101+1081601]\n\tOrdinal0 [0x00B95E8E+1400462]\n\tOrdinal0 [0x00B8639B+1336219]\n\tOrdinal0 [0x00B627A7+1189799]\n\tOrdinal0 [0x00B63609+1193481]\n\tGetHandleVerifier [0x00E35904+1577972]\n\tGetHandleVerifier [0x00EE0B97+2279047]\n\tGetHandleVerifier [0x00D36D09+534521]\n\tGetHandleVerifier [0x00D35DB9+530601]\n\tOrdinal0 [0x00C44FF9+2117625]\n\tOrdinal0 [0x00C498A8+2136232]\n\tOrdinal0 [0x00C499E2+2136546]\n\tOrdinal0 [0x00C53541+2176321]\n\tBaseThreadInitThunk [0x766CFA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x774B7A9E+286]\n\tRtlGetAppContainerNamedObjectPath [0x774B7A6E+238]\n"
     ]
    }
   ],
   "source": [
    "# An array to store all the URL that are being queried\n",
    "imageArr = []\n",
    "\n",
    "# Maximum number of pages one wants to iterate over\n",
    "MAX_PAGE =10\n",
    "\n",
    "# Loop to generate all the URLS.\n",
    "for i in range(0,MAX_PAGE):\n",
    "    totalRecords = 0 if i==0 else (250*i)+1\n",
    "    imdb_url = f'https://www.imdb.com/search/title/?languages=bn&sort=year,desc&user_rating=1.0,10.0&title_type=feature&count=250&start={totalRecords}&ref_=adv_nxt'\n",
    "    imageArr.append(imdb_url)\n",
    "\n",
    "# now we call all the URLs to collect data\n",
    "for bangla_movies_link in imageArr:\n",
    "    get_review(bangla_movies_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('bangla movie user rating dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
