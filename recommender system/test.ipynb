{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspired by https://github.com/PacktPublishing/Python-Machine-Learning-By-Example-Third-Edition/blob/master/chapter2/movie_recommendation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to load rating data from file and also return the number of ratings for each movie and movie_id index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rating_data(data_path, n_users, n_movies):\n",
    "    \"\"\"\n",
    "    Load rating data from file and also return the number of ratings for each movie and movie_id index mapping\n",
    "    @param data_path: path of the rating data file\n",
    "    @param n_users: number of users\n",
    "    @param n_movies: number of movies that have ratings\n",
    "    @return: rating data in the numpy array of [user, movie]; movie_n_rating, {movie_id: number of ratings};\n",
    "             movie_id_mapping, {movie_id: column index in rating data}\n",
    "    \"\"\"\n",
    "    data = np.zeros([n_users, n_movies], dtype=np.float32)\n",
    "    movie_id_mapping = {}\n",
    "    movie_n_rating = defaultdict(int)\n",
    "    with open(data_path, 'r') as file:\n",
    "        for line in file.readlines()[1:]:\n",
    "            user_id, movie_id, rating, _ = line.split(\"::\")\n",
    "            user_id = int(user_id) - 1\n",
    "            if movie_id not in movie_id_mapping:\n",
    "                movie_id_mapping[movie_id] = len(movie_id_mapping)\n",
    "            rating = int(rating)\n",
    "            data[user_id, movie_id_mapping[movie_id]] = rating\n",
    "            if rating > 0:\n",
    "                movie_n_rating[movie_id] += 1\n",
    "    return data, movie_n_rating, movie_id_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "displaying rating info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_distribution(data):\n",
    "    values, counts = np.unique(data, return_counts=True)\n",
    "    for value, count in zip(values, counts):\n",
    "        print(f'Number of rating {int(value)}: {count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading data and displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rating 0: 21384032\n",
      "Number of rating 1: 56174\n",
      "Number of rating 2: 107557\n",
      "Number of rating 3: 261197\n",
      "Number of rating 4: 348971\n",
      "Number of rating 5: 226309\n"
     ]
    }
   ],
   "source": [
    "data_path = 'ml-1m/ratings.dat'\n",
    "n_users = 6040\n",
    "n_movies = 3706\n",
    "data, movie_n_rating, movie_id_mapping = load_rating_data(data_path, n_users, n_movies)\n",
    "\n",
    "display_distribution(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most ratings are unknown, we take the movie with the most known ratings as our target movie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie ID 2858 has 3428 ratings.\n"
     ]
    }
   ],
   "source": [
    "movie_id_most, n_rating_most = sorted(movie_n_rating.items(), key=lambda d: d[1], reverse=True)[0]\n",
    "print(f'Movie ID {movie_id_most} has {n_rating_most} ratings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The movie with ID 2858 is the target movie, and ratings of the rest of the movies are signals. We construct the dataset accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (3428, 3705)\n",
      "Shape of Y: (3428,)\n"
     ]
    }
   ],
   "source": [
    "X_raw = np.delete(data, movie_id_mapping[movie_id_most], axis=1)\n",
    "Y_raw = data[:, movie_id_mapping[movie_id_most]]\n",
    "\n",
    "#We discard samples without a rating in movie ID 2858:\n",
    "X = X_raw[Y_raw > 0]\n",
    "Y = Y_raw[Y_raw > 0]\n",
    "\n",
    "print('Shape of X:', X.shape)\n",
    "print('Shape of Y:', Y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we take a look at the distribution of the target movie ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rating 1: 83\n",
      "Number of rating 2: 134\n",
      "Number of rating 3: 358\n",
      "Number of rating 4: 890\n",
      "Number of rating 5: 1963\n"
     ]
    }
   ],
   "source": [
    "display_distribution(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider movies with ratings greater than 3 as being liked (being recommended):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2853 positive samples and 575 negative samples.\n"
     ]
    }
   ],
   "source": [
    "recommend = 3\n",
    "Y[Y <= recommend] = 0\n",
    "Y[Y > recommend] = 1\n",
    "\n",
    "n_pos = (Y == 1).sum()\n",
    "n_neg = (Y == 0).sum()\n",
    "print(f'{n_pos} positive samples and {n_neg} negative samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a rule of thumb in solving classification problems, we need to always analyze the label distribution and see how balanced (or imbalanced) the dataset is.\n",
    "\n",
    "Next, to comprehensively evaluate our classifier's performance, we can randomly split the dataset into two sets, the training and testing sets, which simulate learning data and prediction data, respectively. Generally, the proportion of the original dataset to include in the testing split can be 20%, 25%, 33.3%, or 40%. We use the train_test_split function from scikit-learn to do the random splitting and to preserve the percentage of samples for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2742 686\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(Y_train), len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the training and testing sizes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2742 686\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_train), len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another good thing about the train_test_split function is that the resulting training and testing sets will have the same class ratio.\n",
    "\n",
    "Next, we train a Naïve Bayes model on the training set. You may notice that the values of the input features are from 0 to 5, as opposed to 0 or 1 in our toy example. Hence, we use the MultinomialNB module (https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) from scikit-learn instead of the BernoulliNB module, as MultinomialNB can work with integer features. We import the module, initialize a model with a smoothing factor of 1.0 and prior learned from the training set, and train this model against the training set as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=1.0, fit_prior=True)\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the trained model to make predictions on the testing set. We get the predicted probabilities as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.50487439e-23 1.00000000e+00]\n",
      " [1.01806208e-01 8.98193792e-01]\n",
      " [3.57740570e-10 1.00000000e+00]\n",
      " [1.00000000e+00 2.94095407e-16]\n",
      " [1.00000000e+00 2.49760836e-25]\n",
      " [7.62630220e-01 2.37369780e-01]\n",
      " [3.47479627e-05 9.99965252e-01]\n",
      " [2.66075292e-11 1.00000000e+00]\n",
      " [5.88493563e-10 9.99999999e-01]\n",
      " [9.71326867e-09 9.99999990e-01]]\n"
     ]
    }
   ],
   "source": [
    "prediction_prob = clf.predict_proba(X_test)\n",
    "print(prediction_prob[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the predicted class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 0. 0. 0. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "prediction = clf.predict(X_test)\n",
    "print(prediction[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the model's performance with classification accuracy, which is the proportion of correct predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: 71.6%\n"
     ]
    }
   ],
   "source": [
    "accuracy = clf.score(X_test, Y_test)\n",
    "print(f'The accuracy is: {accuracy*100:.1f}%')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "639fde48ec0128a88b630484619c8c1a2f485ef59b6c38ec287b184a16f1b08e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
